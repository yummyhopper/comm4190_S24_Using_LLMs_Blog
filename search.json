[
  {
    "objectID": "posts/002_gpt-4-0125-preview_and_laziness/index.html",
    "href": "posts/002_gpt-4-0125-preview_and_laziness/index.html",
    "title": "ChatGPT Got Less Lazy? Open AI’s Newest Model: gpt-4-0125-preview",
    "section": "",
    "text": "ChatGPT Got Less Lazy? Open AI’s Newest Model: gpt-4-0125-preview\n\nOn January 25, 2024 OpenAI released it’s latest and most advanced GPT model to date intended to compete with Google’s Gemini.\n\n\n“This model completes tasks like code generation more thoroughly than the previous preview model and is intended to reduce cases of “laziness” where the model doesn’t complete a task. The new model also includes the fix for the bug impacting non-English UTF-8 generations.”\n\n\nHow can AI be lazy?\nWhen we think next gen AI images of eating pizza rolls and binging Game of Thrones aren’t what comes to mind… but why not? In a december tweet OpenAI acknowledged the issue:\n\n“We’ve heard all your feedback about GPT4 getting lazier! we haven’t updated the model since Nov 11th, and this certainly isn’t intentional. model behavior can be unpredictable, and we’re looking into fixing it.”\n\nBut anyone who’s used ChatGPT has inevitably run into this issue. It’s that keyboard banging moment when you ask it to sort a CSV or write Python code and it might returns a long explanation of how that could be done and recommend the user try to do it themselves. Annoying!\n\n\nWas GPT 3.5 better at this?\nYes! When I asked each model: “Create a spreadheet for my taxes” This is what they returned:\n\n\n\n\n\n\n \n\n\n\nSo is the newest edition really better? We’ll have to wait and see as it’s currently only available throught API.\n\n\nSo what can we conclude?\nThe problem of AI laziness is really a problem of human laziness, but one that developers have to account for. With a little more effortful prompting it would not be difficult for any of the models to deliver an optimal result, but that’s not the AI dream for many users. We are cognitive misers with brains designed to search for solutions that minimize effort while maximizing results, but we don’t want our AI systems to behave the same way. Rather OpenAI’s common user wants to give the least possible input and receive a maximally polished result. This means creating action oriented models designed to interpret simple prompts and infer several layers of extrapolation and nuance to deliver the desired result. Whether frontier AI companies choose to cater to the laziness of the mass rather than creating more nuanced and specialized systems that produce higher quality responses to higher quality prompts can be debated. What’s became clear from this dilemma is that there’s more that goes into a general purpose model than how well it scores on an SAT, the sheer size of its context window, or its fancy new name. To quote Janet Jackson “What have you done for me lately?” The most commercially viable AI systems will tend towards prioritizing user experience, whether that comes at the expense of accuracy or another measure remains to be seen."
  },
  {
    "objectID": "posts/003_SORA/index.html",
    "href": "posts/003_SORA/index.html",
    "title": "Open AI’s SORA: You Can’t Believe What You See…",
    "section": "",
    "text": "Open AI’s SORA: You Can’t Believe What You See…\n\nAs of today OpenAI’s SORA is the newest revelation in a long line of AI breakthroughs. There’s many angles here, but in this post I will explore the safety and misinformation risks of this technology and what Open AI is doing to address them.\n\n\n“Sora is able to generate complex scenes with multiple characters, specific types of motion, and accurate details of the subject and background. The model understands not only what the user has asked for in the prompt, but also how those things exist in the physical world.” - OpenAI\n\n\nI Can’t Believe It!\nGet used to saying that line because OpenAI’s first text-to-video model SORA generates results that not only seem impossible… but often times are. Check out this SORA generated video of a man pondering the history of the universe in a Parisian cafe.\n\nPrompt: An extreme close-up of an gray-haired man with a beard in his 60s, he is deep in thought pondering the history of the universe as he sits at a cafe in Paris, his eyes focus on people offscreen as they walk as he sits mostly motionless, he is dressed in a wool coat suit coat with a button-down shirt , he wears a brown beret and glasses and has a very professorial appearance, and the end he offers a subtle closed-mouth smile as if he found the answer to the mystery of life, the lighting is very cinematic with the golden light and the Parisian streets and city in the background, depth of field, cinematic 35mm film.\n\n\n&lt;source src=closeup-man-in-glasses.mp4e.mp4” type=“video/o tag. \nThe realistic details of his wrinkles and ear piercing, the light illuminating the grizzle in his beard, the shaky camera angle adding character to the shot. But most of all the micro-expression in his face, down to the minutia of his eyes seem to communicate a deep existential awareness despite this person never having existed. So if this is really the type of product we can at some point expect from an OpenAI product… not only is it unbelievable, but how can we ever believe videos we see online again?\n\n\nIs C2PA Metadata A Solution?\nIn addition to existing safety methods such as using text classification to reject harmful prompts, OpenAI boasts of include C2PA Metdata when they eventually release SORA to the public. But what is this jumble of letters and numbers?\n\n“C2PA is an open technical standard that allows publishers, companies, and others to embed metadata in media for verifying its origin and related information.” - OpenAI\n\nTheoretically, this guardrail should be able to keep records of where content came from and how it was modified so bad actors can be held in check. For example, if SORA is used to falsify of a politician engaging in hate speech there would be a surveillance apparatus telling us who produced the video and where they are. More detail on how this works is included in the image below:\n\n\n\nc2pa_visualglossary.png\n\n\nHowever metadata is notoriously unreliable and can easily be removed either intentionally or unintentionally. For example, harmful content that has slithered through other loopholes can easily be replicated through screen recording, and even if a bad actor doesn’t think of this most social media sites automatically remove metadata from uploaded videos to preserve user privacy.\n\n\nSo Will We Be Able To Believe Anything We See Online?\nIn a world where US President Joe Biden posts “Lol hey guys” on his TikTok, and millions of jobs have moved online post-pandemic, it seems like the world is more than ever shaped by the content that appears on screens. As already evidenced by misinformation campaigns, fishing scams, and cases of targeted fraud, scrutiny to what people see online must intensify. And most people won’t be ready for this. One risk is that people who haven’t been duped or don’t know they’ve been duped won’t realize the full extent of the issue. As these models become more potent and omnipresent it seems unavoidable that problems will slip through the cracks. While safety measures will be strongly enforced–as evidenced by the issues with C2PA Metadata–these too must be heavily scrutinized. The sensationalist press and engagement-favoring social media algorithms aren’t going to do all this scrutinization so it’s going to need to come from the individuals consuming content in this new media landscape. I’m excited and apprehensive to see what this will look like in the coming months especially with the upcoming US presidential election."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Explorations with LLMs",
    "section": "",
    "text": "Open AI’s SORA: You Can’t Believe What You See…\n\n\n\n\n\n\nText-to-video\n\n\nSORA\n\n\nC2PA Metadata\n\n\n\nAs of today OpenAI’s SORA is the newest revelation in a long line of AI breakthroughs. There’s many angles here, but in this post I will explore the safety and misinformation risks of this technology and what Open AI is doing to address them.\n\n\n\n\n\nFeb 16, 2024\n\n\nMax Orenstein\n\n\n\n\n\n\n\n\n\n\n\n\nChatGPT Got Less Lazy? Open AI’s Newest Model: gpt-4-0125-preview\n\n\n\n\n\n\nLLMs\n\n\nChatGPT\n\n\nLaziness\n\n\n\nAn example post from a Jupyter notebook\n\n\n\n\n\nFeb 2, 2024\n\n\nAn LLM User\n\n\n\n\n\n\nNo matching items"
  }
]