
@article{goldstein_how_2024,
	title = {How persuasive is {AI}-generated propaganda?},
	volume = {3},
	issn = {2752-6542},
	url = {https://academic.oup.com/pnasnexus/article/doi/10.1093/pnasnexus/pgae034/7610937},
	doi = {10.1093/pnasnexus/pgae034},
	abstract = {Abstract 
            Can large language models, a form of artificial intelligence (AI), generate persuasive propaganda? We conducted a preregistered survey experiment of US respondents to investigate the persuasiveness of news articles written by foreign propagandists compared to content generated by GPT-3 davinci (a large language model). We found that GPT-3 can create highly persuasive text as measured by participants’ agreement with propaganda theses. We further investigated whether a person fluent in English could improve propaganda persuasiveness. Editing the prompt fed to GPT-3 and/or curating GPT-3’s output made GPT-3 even more persuasive, and, under certain conditions, as persuasive as the original propaganda. Our findings suggest that propagandists could use AI to create convincing content with limited effort.},
	language = {en},
	number = {2},
	urldate = {2024-03-29},
	journal = {PNAS Nexus},
	author = {Goldstein, Josh A and Chao, Jason and Grossman, Shelby and Stamos, Alex and Tomz, Michael},
	editor = {Contractor, Noshir},
	month = feb,
	year = {2024},
	pages = {pgae034},
}

@article{chen_can_2023,
	title = {Can {LLM}-{Generated} {Misinformation} {Be} {Detected}?},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2309.13788},
	doi = {10.48550/ARXIV.2309.13788},
	abstract = {The advent of Large Language Models (LLMs) has made a transformative impact. However, the potential that LLMs such as ChatGPT can be exploited to generate misinformation has posed a serious concern to online safety and public trust. A fundamental research question is: will LLM-generated misinformation cause more harm than human-written misinformation? We propose to tackle this question from the perspective of detection difficulty. We first build a taxonomy of LLM-generated misinformation. Then we categorize and validate the potential real-world methods for generating misinformation with LLMs. Then, through extensive empirical investigation, we discover that LLM-generated misinformation can be harder to detect for humans and detectors compared to human-written misinformation with the same semantics, which suggests it can have more deceptive styles and potentially cause more harm. We also discuss the implications of our discovery on combating misinformation in the age of LLMs and the countermeasures.},
	urldate = {2024-03-29},
	author = {Chen, Canyu and Shu, Kai},
	year = {2023},
	keywords = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), Cryptography and Security (cs.CR), FOS: Computer and information sciences, Human-Computer Interaction (cs.HC), Machine Learning (cs.LG)},
}
